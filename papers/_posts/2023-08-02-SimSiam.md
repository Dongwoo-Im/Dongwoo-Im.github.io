---
layout: post
title:  "SimSiam: Exploring Simple Siamese Representation Learning"
date:   2023-08-02 19:00:00 +0900
categories: review
comments: true
use_math: true
sitemap :
    changefreq: daily
    priority: 1.0
---

# [논문리뷰] SimSiam: Exploring Simple Siamese Representation Learning (CVPR '21 Best Paper Honorable Mentions)

[arXiv link](https://arxiv.org/abs/2011.10566)

[github link](https://github.com/facebookresearch/simsiam)

---

## <center> Abstract

Siamese network + two augmentations of one image 구조에 기반하는 self-supervised learning에서 stop-gradient operation이 collapsing 방지에 중요한 역할을 한다. (negative sample, large batch, momentum encoder 보다)

## <center> 1. Introduction

Collapsing이란, similarity를 높이는 방식으로 학습하다보니 각 image에서 good representation을 추출하지 못하고 constant output을 내뱉는 현상을 의미합니다. 이전 연구들인 SimCLR에서는 negative pair를 추가한 contrastive learning을, SwAV에서는 online clustering을, BYOL에서는 momentum encoder를 사용하며 이러한 collapsing을 방지하였습니다.

SimSiam의 저자들은 이러한 솔루션들이 핵심이 아니고 stop-gradient가 중요한 역할을 한다고 주장하며, optimization 관점에서의 가설로 이를 증명합니다. 또한, siamese network 구조는 two augmented input으로부터 invariance feature를 추출하는 inductive bias를 가진다고 언급합니다.

## <center> 3. Method

![Figure 1](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/fig1.webp){: .align-center}

![Algorithm 1](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/alg1.webp){: .align-center}

Figure 1과 pseudo-code를 통해 SimSiam의 방식을 확인할 수 있습니다.

![Expression 1](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp1.webp){: .align-center}

Objective로는 (BYOL의 l2 normed MSE loss와 동일하게) negative cosine similarity를 사용하였으며, symmetrized loss 형태입니다.

- 참고: [Is cosine similarity identical to l2-normalized euclidean distance?](https://stats.stackexchange.com/questions/146221/is-cosine-similarity-identical-to-l2-normalized-euclidean-distance)

## <center> 4. Empirical Study

요약 : stop-gradient를 적용하지 않거나 predictor MLP를 제거하는 경우에만 collapsing 현상이 나타난다. (batch size, batch normalization, similarity function, symmetric loss는 collapsing을 방지하는 주요 factor가 아님을 주장)

### 4.1. Stop-gradient

![Figure 2](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/fig2.webp){: .align-center}

SimSiam 모델에서 stop-gradient 유무에 따른 성능을 비교합니다. Stop-gradient를 적용하지 않는 경우 collapsing되는 것을 확인할 수 있습니다.

또한, 실험 결과에 대한 해석도 덧붙였습니다.

- w/o stop-grad train loss가 -1에 수렴 (= collapsing)
- w/ stop-grad output std가 $1/\sqrt{d}$에 수렴 (output vector z ~ i.i.d $N(0,1)$인 경우, std가 $1/\sqrt{d}$에 근사)

### 4.2. Predictor

![Table 1](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/tab1.webp){: .align-center}

- (a) : predictor가 없는 경우, collapsing
- (b) : predictor를 random weight로 고정한 경우, loss가 수렴하지 않음
- (c) : predictor의 lr을 decay하지 않는 경우, 더 높은 acc (저자들은 predictor가 latest representation을 반영할 수 있어야 하기 때문에, 굳이 lr decay할 필요가 없을 것으로 추측)

### 4.3. Batch Size

![Table 2](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/tab2.webp){: .align-center}

256부터 2048에 이르기까지 batch size에 무관한 성능을 보이고 있으며, 64 또는 128 처럼 낮춘 경우에는 약간의 성능 저하를 확인할 수 있습니다. 높은 batch size(4096)에서의 성능 저하는 SGD optimizer에서 기인한 것으로 볼 수 있습니다.

### 4.4. Batch Normalization

![Table 3](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/tab3.webp){: .align-center}

- (a) : acc가 낮긴 하지만 collapsing을 보이지는 않는다. (optimization에 어려움을 겪는 것으로 추정)
- (b) : hiiden BN 추가
- (c) : projection MLP에 output BN 추가 (output BN의 affine transform을 제거하면 0.1% acc 상승)
- (d) : prediction MLP에 output BN 추가 (collapsing은 아니지만, 학습이 잘 되지 않음)

요약하면 BN을 적절한 위치에 사용하면 학습에 도움이 되지만 collapsing과는 무관하다고 주장합니다.

### 4.5. Similarity Function

![Expression 2](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp2.webp){: .align-center}

![Expression 3](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp3.webp){: .align-center}

저자들은 cosine similarity를 cross-entropy similarity로 대체해봤는데 collapsing 없이 준수한 성능을 보였습니다. 이를 통해 cosine similarity가 collapsing 방지에 중요한 요소가 아니라고 주장합니다. (softmax는 채널 축을 따라 d차원에 대해 동작)

### 4.6. Symmetrization

![Expression 4](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp4.webp){: .align-center}

Asymmetric loss에서도 collapsing 현상은 발견되지 않았고, loss update양을 보정하기 위해 2배를 해주자 symmetric loss와 비슷한 성능을 보입니다.

## <center> 5. Hypothesis

요약 : SimSiam은 alternating optimization으로 설명할 수 있으며, 이러한 가정에 따라 stop-gradient operation의 필요성을 확인할 수 있다. 또한, predictor MLP는 augmentation 분포에 대한 근사를 돕는 역할을 하는 것으로 보이며, momentum encoder가 이러한 역할에 도움되는 것으로 보인다.

### 5.1. Formulation

저자들은 2개의 변수와 2개의 sub-problem으로 구성된 Expectation-Maximization (EM) 알고리즘으로 SimSiam을 설명할 수 있다고 가정합니다. 이를 위해, SimSiam의 objective를 수식으로 재정의하면 다음과 같습니다. (편의를 위해 현재 상황에서 predictor는 고려하지 않으며, 추후 다룰 예정입니다.)

![Expression 5](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp5.webp){: .align-center}

- $\theta$ : encoder network $\mathcal{F}$의 parameter
- $\eta$ : stop-gradient를 설명하기 위한 변수
    - $\theta$처럼 network의 parameter 형태로 표현할 필요가 없기 때문에 $\eta$로 간소화
    - $\eta_{x}$ : representation of image x
- $\mathcal{T}$ : augmentation

전체적으로 image x와 augmentation $\mathcal{T}$ 분포에 대한 expectation이 걸려있는데, 추후 sampling 개념을 적용해서 계산이 가능해집니다. 그리고 그 안에 encoder network $\mathcal{F}$와 $\eta$에 대한 (l2 normed) MSE loss가 존재합니다.

![Expression 6](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp6.webp){: .align-center}

저자들이 EM 알고리즘을 언급한 이유는 위 2개의 sub-problem 수식에서 확인할 수 있습니다. 이는 $\theta$와 $\eta$ 각 변수를 순차적으로 optimize하면 objective의 해를 찾을 수 있다는 관점인데요.

더 나아가, 이러한 alternating algorithm의 형태는 마치 k-means clustering과 유사한 면이 있다고 언급합니다. (이와 함께, 2개의 augmented view로부터 similarity를 높이는 과정이 clustering과 유사하다고 생각할 수도 있을 것 같습니다.)

- $\theta$ = clustering center
- $\eta$ = assignment vector of image x

#### One-step alternation

지금까지 SimSiam이 EM 알고리즘, k-means clustering과 유사하다는 저자들의 가정을 살펴보았고, 위 가정이 SimSiam에 적합하다는 점을 이론적으로 살펴보겠습니다.

먼저, $\eta$를 각 image x에 대해 접근하여 $\eta_x$로 표현하면 아래 수식들을 얻을 수 있습니다.

![Expression 7](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp7.webp){: .align-center}

![Expression 8](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp8.webp){: .align-center}

그리고 augmentation $\mathcal{T}$는 학습 과정에서 sampling되기 때문에 $\mathcal{T}^{\prime}$으로 근사시킬 수 있으며, 이 과정에서 expectation을 무시할 수 있다고 주장합니다. (augmentation 분포를 알지 못하기에 수식 전개를 위한 선택)

![Expression 9](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp9.webp){: .align-center}

마지막으로 위 수식을 첫번쨰 sub-problem인 $\theta$ update 식에 대입하면 아래 수식이 됩니다.

![Expression 10](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp10.webp){: .align-center}

결과적으로 $\eta$는 식에서 사라졌고, $\theta^{t+1}$를 얻기 위해서는 $\theta^t$(= stop-gradient operation)가 필요하다는 사실을 수식으로 확인할 수 있게 됩니다.

#### Predictor

이제 predictor h를 고려할 차례입니다.

![Expression 11](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp11.webp){: .align-center}

![Expression 12](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp12.webp){: .align-center}

앞선 수식 전개 과정에서 augmentation $\mathcal{T}$에 대한 expectation을 무시했는데, 저자들은 predictor h를 통해 augmentation $\mathcal{T}$ 근사 과정이 더 tight해질 수 있다고 주장합니다. (학습을 통해 predictor가 augmentation 분포를 근사하는 것이 가능해질 수 있다는 입장)

#### Symmetrization

마지막으로 symmetric loss를 고려할 차례입니다.

저자들은 symmetrization이 augmentation $\mathcal{T}$에 대한 dense sampling으로 생각할 수 있다고 주장합니다. 즉, $(\mathcal{T}_{1}, \mathcal{T}_{2})$ pair 뿐만 아니라, $(\mathcal{T}_{2}, \mathcal{T}_{1})$ pair도 고려할 수 있게 된다는 내용입니다.

Symmetric loss가 collapsing 방지에 반드시 필요한 것은 아니라는 4.6의 실험 결과가 이를 뒷받침하고 있습니다.

### 5.2. Proof of concept

#### Multi-step alternation

저자들은 k step마다 $\theta$를 update하는 실험을 진행합니다. $\eta_x$는 매 update마다 필요한 만큼 미리 계산하여 memory에 cache하였습니다.

![Expression 13](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp13.webp){: .align-center}

위 표에서 1-step은 SimSiam을 의미하며, step이 많아지더라도 collapsing이 발생하지 않는 것을 확인할 수 있습니다. (SGD optimizer를 사용했기 때문에 가능한 현상 같기도 합니다.)

#### Expectation over augmentations

Expectation over augmentation을 근사(무시)하는 대신에, $\eta_x$에 moving average를 적용해보는 실험입니다. (momentum encoder와 유사)

![Expression 9](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp9.webp){: .align-center}

![Expression 14](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp14.webp){: .align-center}

위 식을 아래 식으로 대체하고 predictor h를 제거하였더니, 55% accuracy를 보였습니다. (momentum coefficient는 0.8을 사용했는데, 보통 0.99 정도의 높은 값을 사용하기 때문에 이에 대한 ablation이 있었으면 좋았을 것 같네요.)

이는 predictor h가 $\mathbb{E}_{\mathcal{T}}$를 근사하는 역할을 수행한다는 점을 뒷받침한다고 주장합니다.

### 5.3. Discussion

지금까지의 가설과 실험은 SimSiam의 optimization이 이루어지는 방식에 대한 것일뿐, 어떻게 collapsing을 방지할 수 있는지에 대해서는 다루지 못했다고 밝힙니다.

이에 대해 저자들은 간단히 생각을 밝혔는데요. 비록 학습 초기에는 $\eta$가 random init network로부터 얻어지므로 constant에 해당하지만, alternating optimizer가 모든 x에 대한 jontly gradient를 계산하는 것이 아니라, 파편화된 $\eta_x$에 대한 gradient를 계산하기 때문에, collapsing 현상이 발생하기 어려웠을 것이라고 추측합니다.

## <center> 6. Comparisons

### 6.1. Result Comparisons

#### ImageNet

![Table 4](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/tab4.webp){: .align-center}

ImageNet linear evaluation 성능입니다. (ResNet50, two 224x224 views, single crop)

100 epoch에서는 좋은 성능을 보이지만, 200, 400, 800 epoch에서는 BYOL에 비해 꽤 낮은 성능을 보이고 있네요. 그럼에도 momentum encoder와 large batch size가 필요하지 않다는 점에서 memory-efficient한 학습 방식이라고 생각됩니다. (memory 관련 figure가 있었다면 더 좋지 않았을까 싶네요.)

#### Transfer Learning

![Table 5](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/tab5.webp){: .align-center}

ImageNet 성능에 비해 SimSiam의 transfer learning 성능은 준수한 것으로 보입니다. 특이한 점은 learning_rate와 weight_decay parameter를 수정하였더니, ImageNet classification 성능에는 별 차이가 없었지만 transfer learning 성능은 크게 상승하였다고 합니다.

### 6.2. Methodology Comparisons

![Figure 3](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/fig3.webp){: .align-center}

SimSiam에 존재하지 않는 개념은 빨간 글씨로 표시되었습니다.

#### Relation to SimCLR : SimCLR without negatives

![Expression 15](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp15.webp){: .align-center}

SimCLR에 predictor와 stop-gradient를 추가해보았지만, 성능은 오히려 하락하였습니다. 이에 대해 SimCLR의 contrastive learning이 SimSiam의 alternating optimization 방식과는 상이하기 때문이라고 추측한다고 합니다.

#### Relation to SwAV : SwAV without online clustering

SwAV를 SimSiam의 형태로 표현하기 위해 다음의 간소화 과정이 필요합니다.

- Shared prototype layer는 siamese encoder에 포함되는 것으로 간주합니다.
- SwAV의 prototype은 gradient가 끊긴 채로 weight normalization 되는데, reproduce 과정에 [Weight Normalization 논문](https://arxiv.org/abs/1602.07868)을 적용하여 end-to-end로 gradient가 흐르게 하면서, original SwAV와 비슷한 결과를 얻었다고 합니다.
- SwAV에서는 similarity 함수로 cross-entropy를 사용합니다.

또한, SwAV에는 Sinkhorn-Knopp (SK) transform을 통해 online clustering이 동작하는데, 균형 있게 clustering될 수 있도록 하는 제약사항이 존재합니다. 즉, 이러한 과정이 SwAV의 collapsing을 방지할 수 있도록 기여하는데, SimSiam에서는 이러한 매커니즘을 설명할 수 있는 개념이 존재하지 않습니다. (쉽게 말하면, SK transform은 SimSiam의 format으로 설명할 수 없다는 내용)

![Expression 16](https://dongwoo-im.github.io/assets/img/posts/2023-08-02-SimSiam/exp16.webp){: .align-center}

SwAV에 대해서는 2개의 ablation을 진행합니다.
- w/ predictor : 성능 소폭 하락
- remove stop-gradient : divergence (clustering based method인 SwAV도 alternating 형태이기 때문에 stop-gradient가 중요한 역할을 하고 있을 수도 있다고 추측)

#### Relation to BYOL : BYOL without momentum encoder

저자들은 $\eta$ sub-problem을 해결하는 과정에서 predictor가 $\mathbb{E}_{\mathcal{T}}$를 근사하는 역할을 한다고 추정하였습니다. 그리고 ablation에서 $\eta$에 대한 moving average가 predictor의 역할을 일부 수행하는 것으로 보이는 결과도 얻었습니다.

이를 종합하여 $\eta$ sub-problem이 gradient-based optimization에 의해 해결될 수 있지 않을까, 그리고 이러한 optimization을 momentum encoder와 연결지을 수 있지 않을까 하는 futurew work를 제안하고 있습니다.

