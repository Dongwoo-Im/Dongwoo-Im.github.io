---
layout: post
title:  "DALL-E: Zero-Shot Text-to-Image Generation"
date:   2023-06-12 10:00:00 +0900
categories: review
comments: true
use_math: true
sitemap :
    changefreq: daily
    priority: 1.0
---

# [논문리뷰] DALL-E: Zero-Shot Text-to-Image Generation (ICML '21 Spotlight)

[arXiv link](https://arxiv.org/abs/2102.12092)

[github link](https://github.com/openai/DALL-E)

---

내 맘대로 요약

- Text 및 Image token(=dVAE encoder output)을 input으로 받아, autoregressive하게 next Image token을 예측하며, 이 token들을 모아 dVAE decoder에 태워 T2I generation task 수행

- 큰 모델에 충분한 양의 데이터로 학습하면, zero-shot 평가에서도 domain-specific 모델만큼의 성능을 보인다.

- 12B transformer 모델의 경제성을 위한 고민들 (Sparse attention, Mixed precision training, Distributed optimization, PowerSGD 등)

---

## <center> 2. Method

저자들의 목표는 transformer 구조를 활용해서 text, image token을 single stream으로 autoregressive하게 학습하는 것입니다.

그렇다면 왜 이미지 pixel이 아니라 token이냐? 이미지의 pixel을 그대로 사용하는 경우 메모리가 매우 많이 필요하고 (PixelCNN++에서 밝힌) high-frequency detail에 모델의 학습이 편향됩니다. 그래서 dVAE encoder로 이미지 token을 얻기 위해 dVAE, prior 순서로 2 stage 학습을 합니다. (like VQ-VAE)

Stage 1에서는 dVAE를 학습합니다. 이때 Encoder는 이미지를 token grid로 압축하면서 (256x256x3 -> 32x32), codebook size K를 8192로 큰 값을 설정하여 손실되는 정보량을 보완할 수 있도록 했습니다.

- 이처럼 token을 줄이는 것은 추후 사용할 transformer의 attention 비용을 줄이기 위함이라고 생각할 수 있으며, 아래의 **figure 1**을 통해 token 만으로도 충분히 이미지 복원이 가능함을 보였습니다.

![Figure 1](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/fig1.webp){: .align-center}

Stage 2에서는 BPE-encoded text token 256개와 dVAE-encoded image token 1024개(=32x32)로 image-text joint 분포를 transformer 모델이 학습하게 됩니다.

생성 과정은 수식을 통해서 확인해볼 수 있습니다. 먼저, 각 module이 modeling하는 분포에 대한 설명입니다.

- $q_{\phi}$ : dVAE encoder (modeling generated image tokens given RGB images)
- $p_{\theta}$ : dVAE decoder (modeling generated RGB images given image tokens)
- $p_{\psi}$ : transformer (modeling joint distribution with text and image)

다음은 생성 모델의 동작 방식을 표현한 수식과 objective인 ELBO 입니다.

![latex1](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/latex1.webp){: .align-center}

![latex2](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/latex2.webp){: .align-center}

- $x$ : image
- $y$ : caption (y is conditionally independent of x given z)
- $z$ : token

참고로, $\beta$는 [beta-VAE](https://arxiv.org/abs/1804.03599)에서 도입된 것입니다.

> $\beta$를 통해 reconstruction과 latent code의 disentanglement를 조절할 수 있습니다. $\beta$가 커질수록, latent code가 disentangle 될 수 있다고 하네요.

### 2.1. Stage One: Learning the Visual Cookbook

Stage 1에서는 dVAE를 학습합니다.

저자들은 initial prior $p_{\psi}$가 codebook (K=8192)에 대한 uniform categorical 분포를 따른다고 설정하였습니다.

- 이는 ELBO 식에서 KL divergence 계산에 활용되며, encoder output이 될 token은 codebook 내에서 uniform하게 선택되기를 바란다고 이해할 수 있습니다.

그리고 $q_{\phi}$는 32x32 grid 각각의 채널이 8192인 logit을 parameter로 하는 categorical 분포를 따른다고 설정하였습니다.

하지만 위 설정을 통해 encoder는 discrete 분포를 modeling하게 되어 backpropagation이 어렵게 됩니다. 이를 해결하고자 VQ-VAE에서는 VQ라는 straight-through estimator를 적용하였지만, dVAE에서는 gumbel-softmax relaxation을 적용했습니다. (why?? VQ-VAE는 encoder와 decoder의 flow가 이어지는데, DALL-E에서는 transformer가 autoregressive하게 생성한 token을 decoding한다는 점에서 VQ 방식이 잘 안되었을 것 같다.)

> relaxation parameter인 $\tau$ 값이 0에 가깝게 작아질수록 one-hot vector와 가까워지고, 커질수록 uniform 분포와 가까워집니다.

### dVAE decoder modeling distribution

추가로, dVAE decoder가 modeling하는 $p_{\theta}$의 likelihood는 log-laplace 분포를 사용하여 평가했습니다. 그 이유는 Appendix A.3 에서 확인할 수 있습니다.

- decoder 학습에 관여하는 reconstruction loss는 일반적으로 L1 or L2 objectvie를 사용합니다. 이는 decoder가 modeling하는 분포가 각각 laplace, gaussian 분포인 경우를 가정하고 있습니다.
- 하지만 pixel value는 0~255 사이에 속하기 때문에 이는 mismatch라고 주장하며, pdf가 (0,1)에서 정의되고 µ와 b를 parameter로 갖는 logit-Laplace distribution을 제안하였습니다.
- 이는 dVAE 학습 과정에서 reconstruction term 계산에 사용되며, dVAE decoder의 output feature map 6개 중 3개는 µ parameter를, 나머지 3개는 b parameter를 구하는 데 사용된다고 합니다.
- 마지막으로, 해당 pdf의 분모에 x*(1-x) term이 존재하여, input value scope [0, 255] 대신 (e, 1-e)를 사용하였으며 e는 0.1을 사용했다고 합니다.

### stability

또한, dVAE 학습의 안정성을 위해 저자들이 중요하게 생각했던 부분은 다음과 같습니다.

- Annealing schedule

    - relaxation parameter인 $\tau$ 값을 1에서 1/16로 annealing하면, relaxed ELBO와 true ELBO 사이의 gap이 거의 없었다고 합니다. (Appendix A.2. Training 참고)

- 1x1 convolution (encoder 마지막, decoder 처음)

    - encoder와 decoder 구성에서 conv의 hidden dim을 줄여놓고 마지막에 1x1 conv로 channel을 키워줄 때, true ELBO에 대한 일반화가 잘 되었다고 합니다.

- activation

    - 아마도 1x1 conv를 적용하기 전에 activation을 한 번 더 적용할 때, 초기 안정성에 도움이 된다는 내용으로 보입니다.

마지막으로, KL weight $\beta$를 6.6으로 키웠을 때 codebook 활용도가 좋으면서, 학습이 끝났을 때 reconstruction error가 매우 낮아졌다고 합니다.

> 이는 앞서 언급했던 $\beta$의 reconstruction-regularization trade-off 개념과는 다른 결과입니다. 이를 두고 저자들은 $\beta$ 값이 작은 경우, 학습 초기에 relaxation noise가 codebook 활용도를 낮춰 ELBO 수렴이 좋지 않았을 것으로 추측하고 있습니다.

### 2.2. Stage Two: Learning the Prior

Stage 2에서는 $\phi$와 $\theta$를 fix 시켜놓고, text & image token을 input으로 transformer decoder로 $\psi$를 modeling합니다.

- Text token : BPE-encoding (10% BPE dropout), 최대 256 token, 16384 vocab size
- Image token : dVAE-encoding, 1024 token, 8192 vocab size

    - stage 2에서는 dVAE logit에 argmax sampling만 적용 (gumbel noise X)
    - ImageNet 실험 결과 overparameterized regime에서는 기존 sampling 방식이 useful regularizer의 기능을 했는데, DALL-E stage 2는 underparameterized regime여서 큰 상관 없을 것이라고 판단했다고 합니다. (dataset과 model이 크기 때문에 underparameterized regime라고 판단한 것으로 추측됩니다.)

### embedding

![Figure 10](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/fig10.webp){: .align-center}

**Figure 10**처럼 text token + pad token + image token 의 순서로 concat 되며, text token에 positional embedding, image token에 row & column embedding이 적용됩니다. 이처럼 pad token을 적용할 경우, [Conceptual Captions](https://aclanthology.org/P18-1238.pdf) 논문에 따르면 validation loss는 높아지지만, OOD caption에 대해 좋은 성능을 보인다고 합니다.

### sparse attention

Transformer decoder에서 사용한 sparse attention mask는 총 64개 layer에 (row + column + [row + row + row + column] * 15 + row + conv) 순서로 적용되었습니다.

> 참고로, 아래의 sparse attention mask의 대전제는 **각 image token이 모든 text token에 attention 될 수 있는가** 였습니다.

![Figure 11](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/fig11.webp){: .align-center}

**figure 11**은 6 text token + 16 image token을 예시로 하는 경우로, text / image token이 아래와 같은 형식으로 concat된 결과입니다.

- Text-to-text attention : standard attention mask
- Image-to-image attention : row, column, or convolutional attention mask

각 예시에 대한 설명은 다음과 같습니다.

- (a) Row attention mask

    - row 1개의 길이보다 1만큼 더 attention 했습니다. (예시 기준, 4 + 1 = 5)
    - 그 이유는 previous row의 same column token까지 attention에 추가하기 위함입니다.

- (b) Colmun attention mask

    - (b)는 GPU 활용도가 낮기 때문에 (c)를 고안하여 사용했습니다.

- (c) Column attention mask with transposed image states

    - (b)에서 row와 column을 trasnpose하면 (c) 형태로 나타난다고 합니다.
    - 아래의 [Sparse Transformer](https://arxiv.org/abs/1904.10509)의 figure를 참고하시면 이해되실 것 같습니다.

- (d) Convolutional attention mask

    - (d)는 3x3 kernel을 사용한 예시로, DALL-E에서는 11x11 kernel을 사용했습니다.

![sparse-fig3](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/sparse-fig3.webp){: .align-center}

마지막으로, DALL-E는 image 생성 모델이기 때문에, **cross-entropy loss 비율을 text(1) : image(7)로 적용**하였습니다.

### 2.3. Data Collection

저자들은 인터넷으로부터 250M 규모의 text-image pair dataset을 구축했습니다.

- MS-COCO train 포함 X (YFCC100M에 포함된 MS-COCO 이미지는 일부 포함될 수 있음)
- Conceptual Captions 포함
- YFCC100M 중 filtered subset 포함
- Wikipedia

자세한 과정은 Appendix C, Conceptual Captions 논문, [DALLE-datasets](https://github.com/robvanvolt/DALLE-datasets) 레포를 참고하시면 도움되실 것 같습니다.

### 2.4. Mixed-Precision Training

DALL-E(12B)보다 작은, 소규모 모델(1B)의 학습 과정에서 GPU 메모리 관리 및 throughput 향상을 위해 다음의 3가지를 설계했습니다.

- 모델 parameter의 대부분, Adam moment, activation을 16bit precision로 저장
- 시간 손해는 있지만 메모리 이점이 있는 activation checkpointing 적용
- 각 transformer block의 backward process에서 activation recompute 

그런데 학습 과정에서 수렴하지 못하는 현상을 발견하였고, 저자들은 16bit precision에서 야기된 underflow 현상을 그 원인으로 보았습니다.

### per-resblock gradient scaling

다음으로 해결책 중 하나였던 **per-resblock gradient scaling**에 대해 설명하겠습니다.

저자들은 [Admin](https://arxiv.org/abs/2004.08249) 논문에서와 유사하게, 앞쪽 block에서 뒤쪽 block으로 전달되는 activation gradient의 norm이 감소하여 underflow 현상이 생겼습니다. (Admin에서 제안하는 initialization을 적용해보았으나 효과는 없었다고 하네요.)

기존의 [Mixed precision training](https://arxiv.org/abs/1710.03740)에서는 smallest, largest activation gradient로 range를 shift하여 이러한 underflow 현상을 극복하려 했습니다. 이는 동일한 크기의 language model 학습을 가능하게 하였지만, text-to-image model (DALL-E)에서는 range가 너무 작았다고 하네요. (표현이 애매하지만, 결론적으로 학습에 방해가 되었다고 해석했습니다.)

그래서 이를 해결하고자 transformer block 별로 graident scaling을 수행하였습니다. Mixed precision training과 비교하면, 모델 전체에 적용되던 scaling을 각 block별로 적용하였다는 점에 있으며, grad scale(=scaling factoer)은 다음과 같이 변화합니다.

- 초기값 : $2^{13}$ * M (M is the number of GPUS)
- 매 update마다 $2^{1/1000}$ (약 1.0007)을 곱해줌
- 만약 nonfinite value가 발생하면, $\sqrt2$를 나눠주고 update를 스킵
- 모든 grad scale은 [$2^{7}$ * M, $2^{24}$ * M] 구간으로 clamp

![fig4](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/fig4.webp){: .align-center}

전체적인 모습은 **figure 4**에서 확인할 수 있습니다. Forward pass는 좌측, backward pass는 우측입니다. 눈여겨볼 부분은 backward 처음의 scale and filter, 마지막의 unscale and filter 입니다. (filter는 Inf나 NaN 같이 backprop 할 수 없는 gradient를 0으로 만드는 과정입니다.) 참고로 TPU + bfloat16 조합에서는 underflow 현상이 없었다고 하네요!

이외에도 Appendix D에 적혀있는 다른 가이드라인들 입니다.

- Only use 16-bit precision where it is really necessary for performance

    - DALL-E 학습에 적용되는 gradient compression 기법인 PowerSGD가 matrix의 rank 분해를 기반으로 하기 때문에, 1D parameter(gain, bias, embedding, unembedding)는 32-bit precision, 32-bit gradient, 32-bit Adam moments를 사용했다고 합니다.

    - Image token, text token의 경우에는 32-bit precision을 적용하였습니다. (해당 표현만 보면 gradient, Adam moment는 16-bit를 사용했으려나 싶네요.) (참고: [encoder](https://github.com/openai/DALL-E/blob/5be4b236bc3ade6943662354117a0e83752cc322/dall_e/encoder.py#L81), [decoder](https://github.com/openai/DALL-E/blob/5be4b236bc3ade6943662354117a0e83752cc322/dall_e/decoder.py#L64))

- Avoid underflow when dividing the gradient

TODO

### 2.5. Distributed Optimization

12B parameter를 갖는 DALL-E 기준으로 16bit precision을 사용하더라도 24GB memory를 사용합니다. 이를 더 낮추기 위해 parameter sharding을 적용했습니다.

![fig5](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/fig5.webp){: .align-center}

TODO

### 2.6. Sample Generation

![fig6](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/fig6.webp){: .align-center}

Caption과 생성된 이미지 사이의 CLIP score를 기반으로 output quality를 reranking 할 수 있음을 보여주고 있습니다.

## <center> 3. Experiments

### 3.1. Quantitative Results

![fig3](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/fig3.webp){: .align-center}

**Figure 3**을 통해 이전의 모델들과의 성능을 비교하고 있는데, DALL-E의 outuput은 512개 중 CLIP score가 가장 높은 결과라는 점도 생각해야 합니다. 다른 모델에도 동일한 process가 적용된 결과도 보여주었으면 좋았을 것 같습니다.

![fig9](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/fig9.webp){: .align-center}

**Figure 9**에서는 (a) MS-COCO, (b) CUB, (c) reranking size 별 성능입니다. (FID는 낮을수록, IS는 높을수록 Good)

- (a) on MS-COCO, (b) on CUB

    - 비록 DALL-E 학습 이미지 중 (a) 21% (b) 12%가 중복되긴 했지만, 해당 dataset의 caption을 사용하지 않았고 실선과 점선(학습된 valid 이미지 제거)에 유의미한 성능 차이는 없었습니다.

    - (a) MS-COCO dataset은 다른 모델들과 비교하여 좋은 성능을 보인다고 할 수 있지만, (b) CUB dataset에서는 zero-shot임을 고려하더라도 유독 좋지 않은 성능을 보이고 있습니다. (CUB dataset 성능 개선 방향으로는 finetuning을 future work로 제안하고 있습니다.)

    - 또한, x축은 blur kernel radius 입니다. 갑자기 blur가 등장한 배경에는 DALL-E가 pixel이 아닌 token을 처리함으로써, high frequency detail 보다는 low frequency information에 집중하기 때문입니다. 즉, DALL-E는 애초에 high frequency detail을 학습하지 않았기 때문에, 생성된 이미지에 blur 처리를 하여 low frequency information 측면에서 다른 모델과 성능을 비교해보자는 아이디어라고 할 수 있습니다. 결과적으로 더 강한 blur가 적용될수록 다른 모델보다 DALL-E의 성능이 상대적으로 좋아지는 경향을 확인할 수 있습니다.

- (c) : Reranking size는 32까지 성능과 비례하는 모습을 보이며, 128 이상인 경우에는 오히려 성능이 하락하는 측면도 확인해볼 수 있습니다.

### 3.2. Data Overlap Analysis

CLIP의 중복 제거 방식을 사용했습니다. 중복 제거를 위한 contrastive model을 사용하여 closeness score를 계산하였고, false negative rate를 낮추는 방향으로 threshold를 직접 정했다고 합니다.

### 3.3. Qualitative Findings

![fig2](https://dongwoo-im.github.io/assets/img/posts/2023-06-12-DALLE/fig2.webp){: .align-center}

- (a) : 실존하지 않는 추상적인 개념에 대한 생성이 가능한 것으로 보입니다.
- (b) : 개념들 간의 관계도 이미지 상에 나타낼 수 있는 것으로 보입니다. (일관적인 성능을 보장하지는 않는다고 언급합니다.)
- (c) : DALL-E에서 글자 생성은 잘 안되는 것으로 알려져 있습니다.
- (d) : 입력된 text를 기반으로 Image-to-Image translation도 가능합니다.

### Reference

- [lucidrains' DALLE-pytorch](https://github.com/lucidrains/DALLE-pytorch)

- [Gumbel-Softmax 리뷰](https://kaen2891.tistory.com/81)

- [Why is Laplace prior producing sparse solutions?](https://stats.stackexchange.com/questions/177210/why-is-laplace-prior-producing-sparse-solutions)

- [Mixed-Precision-Training](https://hoya012.github.io/blog/Mixed-Precision-Training/)